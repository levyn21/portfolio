<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <link href="styles.css" rel="stylesheet">
</head>
<body>
    <header>
        <a href ="#SpeculumCerebri">Speculum Cerebri</a>
        <a href ="#UltrasonicCamera">Ultrasonic Camera</a>
        <a href ="#PredictionMachine">Prediction Machine</a>
        <a href ="#Contact">Contact</a>
    </header>

    <landing>
        <h1>Noah Levy</h1>
        <h2>Interactive Media Artist & Creative Technologist</h2>
    </landing>

    <main>
        <SC>
            <p></p>
            <h3 id="SpeculumCerebri">Speculum Cerebri</h3>
            <img src="https://noahlevyart.weebly.com/uploads/8/4/8/7/84879864/mona-lisa-da-vinci_orig.jpg" alt="Speculum Cerebri - Mona Lisa">
            <img src="https://noahlevyart.weebly.com/uploads/8/4/8/7/84879864/cherity-salviti_orig.jpg" alt="Speculum Cerebri - Cherity">
            <img src="https://noahlevyart.weebly.com/uploads/8/4/8/7/84879864/madonna-raphael_orig.jpg" alt="Speculum Cerebri - Madonna">
            <div><strong>Medium:</strong> biosensing, generative systems, TouchDesigner, Python, OpenBCI, OSC, real-time data visualization</div>
            <div><strong>Practice:</strong> End-to-end design and development of interactive art systems integrating hardware, software, and user experience</div>
            <p>Speculum Cerebri is an interactive installation that explores the relationship between human perception and machine vision. The piece uses a combination of computer vision, projection mapping, and real-time data processing to create a dynamic environment that responds to the presence and movements of viewers. As participants move through the space, they trigger changes in the projected visuals, which are generated based on their interactions. The installation aims to challenge traditional notions of seeing and being seen, inviting viewers to consider how technology mediates our experience of reality.</p>
            <img src="https://noahlevyart.weebly.com/uploads/8/4/8/7/84879864/screenshot-2025-07-21-095347_orig.png" alt="Speculum Cerebri - Diagram">
            <p></p>
        </SC>
        <UC>
            <p></p>
            <h3 id="UltrasonicCamera">Ultrasonic Camera</h3>
            <img src="https://noahlevyart.weebly.com/uploads/8/4/8/7/84879864/background-images/1838316756.png" alt="Ultrasonic Camera - Installation View">
            <div><strong>Medium:</strong> Raspberry Pi, HC-SR04 ultrasonic sensing, Processing (Java), Python, real-time 3D visualization</div>
            <div><strong>Practice:</strong> End-to-end design and development of a sound-based imaging system translating photographic principles into ultrasonic spatial capture</div>
            <p>This project reimagines the photographic process using ultrasonic sensing rather than light. An ultrasonic sensor mounted on a Raspberry Pi captures continuous distance measurements, which are streamed into a Processing program that builds interactive 3D point, line, and shape visualizations. Users can navigate and capture these simulated “images” in real time. By reframing light-based controls into ultrasonic parameters, the work explores how image-making can shift from optical to spatial perception, creating a new way to “photograph” the environment through sound. Photographic principles are translated into sound-based equivalents:</p>
            <div><strong>Sound-Based Photographic Equivalents</strong></div>
            <table>
                <tr>
                    <th scope="col">Photographic Principle</th>
                    <th scope="col">Ultrasonic Equivalent</th>
                </tr>
                <tr>
                    <th scope="row">Aperture</th>
                    <td>Sensor beam width</td>
                </tr>
                <tr>
                    <th scope="row">Shutter Speed</th>
                    <td>Pulse emission frequency</td>
                </tr>
                <tr>
                    <th scope="row">ISO</th>
                    <td>Echo sensitivity threshold</td>
                </tr>
            </table>
            <p></p>
        </UC>
        <PM>
            <p></p>
            <h3 id="PredictionMachine">Prediction Machine</h3>
            <div class="PM-Images">
                <img src="Images/Screenshot 2025-10-06 132755.png" alt="Prediction Machine - Installation View 1">
                <img src="Images/Screenshot 2025-10-06 132816.png" alt="Prediction Machine - Installation View 2">
            </div>
            <div><strong>Medium:</strong> Processing (Java), OpenCV library, webcam video capture, face and movement detection, breath tracking via optical flow</div>
            <div><strong>Practice:</strong> Interactive computer-vision artwork focused on face tracking, optical flow, and probabilistic error modeling to explore perception, prediction, and trust between human and machine</div>
            <p>Inspired by Ted Chiang’s idea that technology reflects the boundaries of human understanding rather than transcending them. The piece examines how machine learning algorithms can both enhance and limit our perception of reality, ultimately questioning the reliability of technology in interpreting human behavior. By translating the act of observation into a probabilistic game, Prediction Machine exposes how easily precision can disguise uncertainty, and how faith in a prediction can matter more than accuracy itself.</p>
            <div><strong>Interaction flow:</strong></div>
            <ol>
                <li>Camera Tracking</li>
                    <ul>
                    <li>The webcam activates and continuously tracks the participant’s face using OpenCV.</li>
                    <li>A green rectangle outlines the detected face, and nose position is monitored to detect left or right motion.</li>
                    </ul>
                <li>Countdown Phase</li>
                    <ul>
                    <li>A five-second timer appears in the center of the screen before each prediction.</li>
                    <li>During this time, all previous movement states reset as the system prepares for the next reading.</li>
                    </ul>
                <li>Movement Prompt</li>
                    <ul>
                    <li>When the timer reaches zero, the word <em>Move!</em> flashes on screen.</li>
                    <li>The participant tilts their head left or right while the program records the motion direction.</li>
                    </ul>
                <li>Prediction and Adjustment</li>
                    <ul>
                    <li>A Markov chain prediction model compares the current motion pattern with previous states.</li>
                    <li>The system updates transition probabilities over time, improving accuracy with each interaction.</li>
                    <li>A blue arrow appears to show the predicted movement direction.</li>
                    </ul>
                <li>Result Marking</li>
                    <ul>
                    <li>The program evaluates the prediction against the participant’s actual movement.</li>
                    <li>A black circle appears for a correct prediction; a red X for an incorrect one.</li>
                    <li>All marks remain visible, creating a layered visual record of accuracy and error.</li>
                    </ul>
                <li>Breath Tracking and Termination</li>
                    <ul>
                    <li>Optical flow detects chest movement to estimate breathing cycles.</li>
                    <li>Each complete breath increases the counter; after twenty-five breaths, the program ends automatically.</li>
                    <li>The final message appears: <em>“You took 25 breaths, so the prediction machine has stopped.”</em></li>
                    </ul>
                </ol>
                <p></p>
        </PM>
        <form>
            <h3 id="Contact">Contact</h3>
            <label for="name">Name:</label>
            <input type="text" id="name" name="name">
            <label for="email">Email:</label>
            <input type="email" id="email" name="email">
            <button type="submit">Subscribe</button>
        </form>

    </main>
    <script src="script.js"></script>
</body>
</html>

